{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import math\n",
    "from random import shuffle\n",
    "import os\n",
    "import sox\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining some constants\n",
    "input_dim = 500\n",
    "numexamples = 8000\n",
    "num_classes = 4\n",
    "alpha = 0.0001\n",
    "num_epochs = 200\n",
    "batch_size = 512\n",
    "classificationweight = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a list of where the 1 indicates the genre\n",
    "# encodes the labels\n",
    "def get_one_hot(label_num, num_classes = 4):\n",
    "    one_hot = np.zeros((1,num_classes))\n",
    "    one_hot[0, int(label_num)] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads the data for processing\n",
    "# preprocesses the data\n",
    "def load_data():\n",
    "\tprint('Reading data...')\n",
    "\ttfm = sox.Transformer()\n",
    "\tsongs = np.zeros((numexamples, input_dim))\n",
    "\tonehotlabels = np.zeros((numexamples, num_classes))\n",
    "\tcounter = 0\n",
    "\t# used four classes\n",
    "\t# jazz removed due to corrupt files in the dataset\n",
    "\tallgenres = ['classical', 'country', 'metal', 'pop']\n",
    "\n",
    "\t# splits of 1 second each\n",
    "\tnumsplit = 20\n",
    "\tsizesplit = input_dim\n",
    "\tfor index in range(len(allgenres)):\n",
    "\t\tfor filename in os.listdir('./genres_original/' + allgenres[index]):\n",
    "\t\t\tif filename.endswith(\".wav\"):\n",
    "\t\t\t\taudio, sr = librosa.core.load('./genres_original/' + allgenres[index] + '/' + filename)\n",
    "\t\t\t\t# creates a samling rate of 500Hz for each song by taking mean of values\n",
    "\t\t\t\taudio = audio[:600000]\n",
    "\t\t\t\taudio = audio.reshape(15000, 40)\n",
    "\t\t\t\taudio = np.mean(audio, axis=1)\n",
    "\n",
    "\t\t\t\tfor j in range(numsplit):\n",
    "\t\t\t\t\tsongs[counter] = audio[(sizesplit * j) : (sizesplit * (j + 1))]\n",
    "\t\t\t\t\tonehotlabels[counter] = get_one_hot(index)\n",
    "\t\t\t\t\tcounter += 1\n",
    "\tsongs = pd.DataFrame(songs)\n",
    "\tonehotlabels = pd.DataFrame(onehotlabels)\n",
    "\tprint('Data reading done :)')\n",
    "\treturn songs, onehotlabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_placeholders returns the placeholders for the input and output data and the dropout rate for the network \n",
    "def get_placeholders():\n",
    "\tinputs_placeholder = tf.compat.v1.placeholder(tf.float32, (None, input_dim))\n",
    "\tlabels_placeholder = tf.compat.v1.placeholder(tf.float32, (None, num_classes))\n",
    "\ttf.compat.v1.add_to_collection('inputs_placeholder', inputs_placeholder)\n",
    "\ttf.compat.v1.add_to_collection('labels_placeholder', labels_placeholder)\n",
    "\tkeep_prob = tf.compat.v1.placeholder_with_default(1.0, shape=(), name='keep_prob')\n",
    "\treturn inputs_placeholder, labels_placeholder, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adds weights and biases to the network and returns the weights and biases for the network\n",
    "def add_parameters():\n",
    "\tweights = {}\n",
    "\t# encoder for the first layer of the network with 500 input nodes and 100 hidden nodes\n",
    "\tweights[\"W1_encoder\"] = tf.compat.v1.get_variable(name=\"W1_encoder\", shape = (input_dim, 256), initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "\tweights[\"W2_encoder\"] = tf.compat.v1.get_variable(name=\"W2_encoder\", shape = (256, 192), initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "\tweights[\"W3_encoder\"] = tf.compat.v1.get_variable(name=\"W3_encoder\", shape = (192, 128), initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "\tweights[\"W4_encoder\"] = tf.compat.v1.get_variable(name=\"W4_encoder\", shape = (128, 64), initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "\n",
    "\t# decoder for the first layer of the network with 100 hidden nodes and 500 output nodes\n",
    "\tweights[\"W1_decoder\"] = tf.compat.v1.get_variable(name=\"W1_decoder\", shape = (64, 128), initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "\tweights[\"W2_decoder\"] = tf.compat.v1.get_variable(name=\"W2_decoder\", shape = (128, 192), initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "\tweights[\"W3_decoder\"] = tf.compat.v1.get_variable(name=\"W3_decoder\", shape = (192, 256), initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "\tweights[\"W4_decoder\"] = tf.compat.v1.get_variable(name=\"W4_decoder\", shape = (256, input_dim), initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "\n",
    "\t# encoder for the second layer of the network with 100 input nodes and 50 hidden nodes\n",
    "\tweights[\"b1_encoder\"] = tf.compat.v1.get_variable(name=\"b1_encoder\", initializer = tf.zeros((1,256)))\n",
    "\tweights[\"b2_encoder\"] = tf.compat.v1.get_variable(name=\"b2_encoder\", initializer = tf.zeros((1,192)))\n",
    "\tweights[\"b3_encoder\"] = tf.compat.v1.get_variable(name=\"b3_encoder\", initializer = tf.zeros((1,128)))\n",
    "\tweights[\"b4_encoder\"] = tf.compat.v1.get_variable(name=\"b4_encoder\", initializer = tf.zeros((1,64)))\n",
    "\n",
    "\t# decoder for the second layer of the network with 50 hidden nodes and 100 output nodes\n",
    "\tweights[\"b1_decoder\"] = tf.compat.v1.get_variable(name=\"b1_decoder\", initializer = tf.zeros((1,128)))\n",
    "\tweights[\"b2_decoder\"] = tf.compat.v1.get_variable(name=\"b2_decoder\", initializer = tf.zeros((1,192)))\n",
    "\tweights[\"b3_decoder\"] = tf.compat.v1.get_variable(name=\"b3_decoder\", initializer = tf.zeros((1,256)))\n",
    "\tweights[\"b4_decoder\"] = tf.compat.v1.get_variable(name=\"b4_decoder\", initializer = tf.zeros((1, input_dim)))\n",
    "\n",
    "\t# softmax classifier weights\n",
    "\tweights[\"W1_softmax\"] = tf.compat.v1.get_variable(name=\"W1_softmax\", shape = (64, 32), initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "\tweights[\"b1_softmax\"] = tf.compat.v1.get_variable(name=\"b1_softmax\", initializer = tf.zeros((1,32)))\n",
    "\tweights[\"W2_softmax\"] = tf.compat.v1.get_variable(name=\"W2_softmax\", shape = (32, 16), initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "\tweights[\"b2_softmax\"] = tf.compat.v1.get_variable(name=\"b2_softmax\", initializer = tf.zeros((1,16)))\n",
    "\tweights[\"W3_softmax\"] = tf.compat.v1.get_variable(name=\"W3_softmax\", shape = (16, num_classes), initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "\tweights[\"b3_softmax\"] = tf.compat.v1.get_variable(name=\"b3_softmax\", initializer = tf.zeros((1,num_classes)))\n",
    "\treturn weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder returns the hidden layer output and the weights of the hidden layer\n",
    "def encoder(inputs_batch, weights, keep_prob):\n",
    "\ta_1 = tf.nn.sigmoid(tf.add(tf.matmul(inputs_batch, weights[\"W1_encoder\"]),weights[\"b1_encoder\"]))\n",
    "\ta_2 = tf.nn.tanh(tf.add(tf.matmul(a_1, weights[\"W2_encoder\"]),weights[\"b2_encoder\"]))\n",
    "\ta_2 = tf.nn.dropout(a_2, rate=1 - (keep_prob))\n",
    "\ta_3 = tf.nn.relu(tf.add(tf.matmul(a_2, weights[\"W3_encoder\"]),weights[\"b3_encoder\"]))\n",
    "\ta_4 = tf.nn.relu(tf.add(tf.matmul(a_3, weights[\"W4_encoder\"]),weights[\"b4_encoder\"]))\n",
    "\treturn a_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder returns the reconstructed input and the weights of the hidden layer\n",
    "def decoder(inputs_batch, weights, keep_prob):\n",
    "\ta_5 = tf.nn.sigmoid(tf.add(tf.matmul(inputs_batch, weights[\"W1_decoder\"]),weights[\"b1_decoder\"]))\n",
    "\ta_6 = tf.nn.sigmoid(tf.add(tf.matmul(a_5, weights[\"W2_decoder\"]),weights[\"b2_decoder\"]))\n",
    "\ta_6 = tf.nn.dropout(a_6, rate=1 - (keep_prob))\n",
    "\ta_7 = tf.nn.relu(tf.add(tf.matmul(a_6, weights[\"W3_decoder\"]),weights[\"b3_decoder\"]))\n",
    "\ta_8 = tf.nn.relu(tf.add(tf.matmul(a_7, weights[\"W4_decoder\"]),weights[\"b4_decoder\"]))\n",
    "\treturn a_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax classifier returns the output of the softmax classifier (Convention of h for hidden layers of classifier)\n",
    "def softmaxclassifier(inputs_batch, weights, keep_prob):\n",
    "\th_1  = tf.nn.tanh(tf.add(tf.matmul(inputs_batch, weights[\"W1_softmax\"]), weights[\"b1_softmax\"]))\n",
    "\th_2  = tf.nn.tanh(tf.add(tf.matmul(h_1, weights[\"W2_softmax\"]), weights[\"b2_softmax\"]))\n",
    "\n",
    "\t# Remove softmax from here\n",
    "\th_3 = tf.add(tf.matmul(h_2, weights[\"W3_softmax\"]), weights[\"b3_softmax\"])\n",
    "\treturn h_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_batch returns a batch of data and labels\n",
    "def get_batches(seq, size=batch_size):\n",
    "    return [seq[pos:pos + size] for pos in range(0, len(seq), size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trains the network and returns the trained weights of the network and the loss of the network for each epoch \n",
    "def train(X, Y, X_dev, Y_dev):\n",
    "\ttf.compat.v1.reset_default_graph()\n",
    "\tinputs_batch, labels_batch, keep_prob = get_placeholders()\n",
    "\tweights = add_parameters()\n",
    "\tencoding = encoder(inputs_batch, weights, keep_prob)\n",
    "\tdecoding = decoder(encoding, weights, keep_prob)\n",
    "\ttf.compat.v1.add_to_collection(\"encoding\", encoding)\n",
    "\ttf.compat.v1.add_to_collection(\"decoding\", decoding)\n",
    "\ty_hat = softmaxclassifier(encoding, weights, keep_prob)\n",
    "\ttf.compat.v1.add_to_collection(\"y_hat\", y_hat)\n",
    "\t# checks shape of y_hat: need to be shape (batch_size, num_classes) according to documentation\n",
    "\tloss = tf.reduce_mean(input_tensor=tf.pow(decoding - inputs_batch, 2)) + tf.reduce_mean(input_tensor=(classificationweight * tf.nn.softmax_cross_entropy_with_logits(labels=tf.stop_gradient(labels_batch), logits=y_hat)))\n",
    "\ttf.compat.v1.add_to_collection(\"loss\", loss)\n",
    "\toptimizer = tf.compat.v1.train.AdamOptimizer(learning_rate = alpha).minimize(loss)\n",
    "\tinit = tf.compat.v1.global_variables_initializer()\n",
    "\tsaver = tf.compat.v1.train.Saver(max_to_keep=5)\n",
    "\twith tf.compat.v1.Session() as sess:\n",
    "\t\tsummary_writer = tf.compat.v1.summary.FileWriter('./tensorboardlogs/softmaxautoencoder', sess.graph)\n",
    "\t\tsess.run(init)\n",
    "\t\t# Shuffling training set\n",
    "\t\tind_list=[i for i in range(X.shape[0])]\n",
    "\t\tshuffle(ind_list)\n",
    "\t\tX = X.iloc[ind_list]\n",
    "\t\tY = Y.iloc[ind_list]\n",
    "\t\ttrain_accuracies = []\n",
    "\t\tdev_accuracies = []\n",
    "\t\tloss_per_epoch = []\n",
    "\n",
    "\t\tfor iteration in range(num_epochs):\n",
    "\t\t\tinputs_batches = get_batches(X)\n",
    "\t\t\tlabels_batches = get_batches(Y)\n",
    "\t\t\tcost_list = []\n",
    "\t\t\tcurrnumcorrect = 0\n",
    "\t\t\tfor i in range(len(inputs_batches)):\n",
    "\t\t\t\tbatch = inputs_batches[i]\n",
    "\t\t\t\tbatchlabel = labels_batches[i]\n",
    "\t\t\t\tbottleneck, reconstruction, preds, _, curr_loss = sess.run([encoding, decoding, y_hat, optimizer, loss], feed_dict={inputs_batch: batch, labels_batch: batchlabel, keep_prob : 0.8})\n",
    "\t\t\t\t# checks if the index of the max value of the prediction is the same as the index of the max value of the label (i.e. if the prediction is correct)\n",
    "\t\t\t\tpredictions = tf.math.argmax(input=preds, axis=1)\n",
    "\t\t\t\ttruelabels = tf.math.argmax(input=batchlabel, axis=1)\n",
    "\t\t\t\tnumequal = tf.math.equal(predictions, truelabels)\n",
    "\t\t\t\tnumcorrect = tf.math.count_nonzero(numequal)\n",
    "\t\t\t\tcurrnumcorrect += numcorrect.eval()\n",
    "\t\t\t\tcost_list.append(curr_loss)\n",
    "\n",
    "\t\t\taccuracy = currnumcorrect / float(X.shape[0])\n",
    "\t\t\tprint(\"Epoch \" + str(iteration+1) + \", Train Accuracy: \" + str(accuracy))\n",
    "\t\t\t_, preds = sess.run([encoding, y_hat], feed_dict={inputs_batch : X_dev, labels_batch : Y_dev, keep_prob : 1.0})\n",
    "\t\t\tpredictions = tf.math.argmax(input=preds, axis=1)\n",
    "\t\t\ttruelabels = tf.math.argmax(input=Y_dev, axis=1)\n",
    "\t\t\tnumequal = tf.math.equal(predictions, truelabels)\n",
    "\t\t\tnumcorrect = tf.math.count_nonzero(numequal)\n",
    "\t\t\tdevaccuracy = numcorrect.eval() / float(X_dev.shape[0])\n",
    "\t\t\tprint(\"Epoch \" + str(iteration+1) + \", Dev Accuracy: \" + str(devaccuracy))\n",
    "\t\t\ttrain_accuracies.append(accuracy)\n",
    "\t\t\tdev_accuracies.append(devaccuracy)\n",
    "\t\t\ttrain_smoothed_cost = float(sum(cost_list)) / len(cost_list)\n",
    "\t\t\tloss_per_epoch.append(train_smoothed_cost)\n",
    "\t\t\tsaver.save(sess, './modelWeights/softmaxautoencoder', global_step = (iteration+1))\n",
    "\t\t\tobjectives_summary = tf.compat.v1.Summary()\n",
    "\t\t\tobjectives_summary.value.add(tag='train_accuracy', simple_value=accuracy)\n",
    "\t\t\tobjectives_summary.value.add(tag='dev_accuracy', simple_value=devaccuracy)\n",
    "\t\t\tobjectives_summary.value.add(tag='train_smoothed_cost', simple_value=train_smoothed_cost)\n",
    "\t\t\tsummary_writer.add_summary(objectives_summary, iteration+1)\n",
    "\t\t\tsummary_writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Data reading done :)\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "songs, labels = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuufles the data\n",
    "ind_list=[i for i in range(songs.shape[0])]\n",
    "shuffle(ind_list)\n",
    "songs = songs.iloc[ind_list]\n",
    "labels = labels.iloc[ind_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits the data into training and dev sets\n",
    "songs_train = songs.iloc[0:6000]\n",
    "songs_dev = songs.iloc[6000:]\n",
    "labels_train = labels.iloc[0:6000]\n",
    "labels_dev = labels.iloc[6000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writes the data to csv files\n",
    "songs_dev.to_csv('songs_dev.csv', index = False)\n",
    "labels_dev.to_csv('labels_dev.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-01 12:39:31.271391: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-01 12:39:31.272490: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-01 12:39:31.272570: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-01 12:39:31.272711: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-01 12:39:31.273009: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-12-01 12:39:31.273330: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-12-01 12:39:31.273658: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-01 12:39:31.273978: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-01 12:39:31.274200: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-12-01 12:39:31.274229: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-12-01 12:39:31.274683: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-01 12:39:31.353435: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Accuracy: 0.2505\n",
      "Epoch 1, Dev Accuracy: 0.2815\n",
      "Epoch 2, Train Accuracy: 0.2355\n",
      "Epoch 2, Dev Accuracy: 0.258\n",
      "Epoch 3, Train Accuracy: 0.24966666666666668\n",
      "Epoch 3, Dev Accuracy: 0.25\n",
      "Epoch 4, Train Accuracy: 0.25783333333333336\n",
      "Epoch 4, Dev Accuracy: 0.2365\n",
      "Epoch 5, Train Accuracy: 0.24616666666666667\n",
      "Epoch 5, Dev Accuracy: 0.241\n",
      "Epoch 6, Train Accuracy: 0.253\n",
      "Epoch 6, Dev Accuracy: 0.2485\n",
      "WARNING:tensorflow:From /home/sunny/.local/lib/python3.10/site-packages/tensorflow/python/training/saver.py:1064: remove_checkpoint (from tensorflow.python.checkpoint.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "Epoch 7, Train Accuracy: 0.25266666666666665\n",
      "Epoch 7, Dev Accuracy: 0.2245\n",
      "Epoch 8, Train Accuracy: 0.25283333333333335\n",
      "Epoch 8, Dev Accuracy: 0.2135\n",
      "Epoch 9, Train Accuracy: 0.25433333333333336\n",
      "Epoch 9, Dev Accuracy: 0.246\n",
      "Epoch 10, Train Accuracy: 0.24983333333333332\n",
      "Epoch 10, Dev Accuracy: 0.2455\n",
      "Epoch 11, Train Accuracy: 0.25833333333333336\n",
      "Epoch 11, Dev Accuracy: 0.255\n",
      "Epoch 12, Train Accuracy: 0.25933333333333336\n",
      "Epoch 12, Dev Accuracy: 0.2575\n",
      "Epoch 13, Train Accuracy: 0.25633333333333336\n",
      "Epoch 13, Dev Accuracy: 0.2645\n",
      "Epoch 14, Train Accuracy: 0.252\n",
      "Epoch 14, Dev Accuracy: 0.254\n",
      "Epoch 15, Train Accuracy: 0.25916666666666666\n",
      "Epoch 15, Dev Accuracy: 0.2565\n",
      "Epoch 16, Train Accuracy: 0.25533333333333336\n",
      "Epoch 16, Dev Accuracy: 0.248\n",
      "Epoch 17, Train Accuracy: 0.2695\n",
      "Epoch 17, Dev Accuracy: 0.257\n",
      "Epoch 18, Train Accuracy: 0.27316666666666667\n",
      "Epoch 18, Dev Accuracy: 0.254\n",
      "Epoch 19, Train Accuracy: 0.2565\n",
      "Epoch 19, Dev Accuracy: 0.2665\n",
      "Epoch 20, Train Accuracy: 0.25733333333333336\n",
      "Epoch 20, Dev Accuracy: 0.259\n",
      "Epoch 21, Train Accuracy: 0.272\n",
      "Epoch 21, Dev Accuracy: 0.2525\n",
      "Epoch 22, Train Accuracy: 0.26216666666666666\n",
      "Epoch 22, Dev Accuracy: 0.251\n",
      "Epoch 23, Train Accuracy: 0.27216666666666667\n",
      "Epoch 23, Dev Accuracy: 0.2555\n",
      "Epoch 24, Train Accuracy: 0.2745\n",
      "Epoch 24, Dev Accuracy: 0.2415\n",
      "Epoch 25, Train Accuracy: 0.2781666666666667\n",
      "Epoch 25, Dev Accuracy: 0.2355\n",
      "Epoch 26, Train Accuracy: 0.28633333333333333\n",
      "Epoch 26, Dev Accuracy: 0.238\n",
      "Epoch 27, Train Accuracy: 0.2855\n",
      "Epoch 27, Dev Accuracy: 0.2515\n",
      "Epoch 28, Train Accuracy: 0.294\n",
      "Epoch 28, Dev Accuracy: 0.2295\n",
      "Epoch 29, Train Accuracy: 0.296\n",
      "Epoch 29, Dev Accuracy: 0.2495\n",
      "Epoch 30, Train Accuracy: 0.3006666666666667\n",
      "Epoch 30, Dev Accuracy: 0.4\n",
      "Epoch 31, Train Accuracy: 0.2941666666666667\n",
      "Epoch 31, Dev Accuracy: 0.384\n",
      "Epoch 32, Train Accuracy: 0.30933333333333335\n",
      "Epoch 32, Dev Accuracy: 0.258\n",
      "Epoch 33, Train Accuracy: 0.31016666666666665\n",
      "Epoch 33, Dev Accuracy: 0.272\n",
      "Epoch 34, Train Accuracy: 0.31483333333333335\n",
      "Epoch 34, Dev Accuracy: 0.368\n",
      "Epoch 35, Train Accuracy: 0.31633333333333336\n",
      "Epoch 35, Dev Accuracy: 0.424\n",
      "Epoch 36, Train Accuracy: 0.3185\n",
      "Epoch 36, Dev Accuracy: 0.38\n",
      "Epoch 37, Train Accuracy: 0.32266666666666666\n",
      "Epoch 37, Dev Accuracy: 0.414\n",
      "Epoch 38, Train Accuracy: 0.3295\n",
      "Epoch 38, Dev Accuracy: 0.4125\n",
      "Epoch 39, Train Accuracy: 0.3285\n",
      "Epoch 39, Dev Accuracy: 0.416\n",
      "Epoch 40, Train Accuracy: 0.33816666666666667\n",
      "Epoch 40, Dev Accuracy: 0.43\n",
      "Epoch 41, Train Accuracy: 0.336\n",
      "Epoch 41, Dev Accuracy: 0.4185\n",
      "Epoch 42, Train Accuracy: 0.3476666666666667\n",
      "Epoch 42, Dev Accuracy: 0.432\n",
      "Epoch 43, Train Accuracy: 0.35233333333333333\n",
      "Epoch 43, Dev Accuracy: 0.4195\n",
      "Epoch 44, Train Accuracy: 0.366\n",
      "Epoch 44, Dev Accuracy: 0.414\n",
      "Epoch 45, Train Accuracy: 0.37466666666666665\n",
      "Epoch 45, Dev Accuracy: 0.3365\n",
      "Epoch 46, Train Accuracy: 0.39916666666666667\n",
      "Epoch 46, Dev Accuracy: 0.408\n",
      "Epoch 47, Train Accuracy: 0.4065\n",
      "Epoch 47, Dev Accuracy: 0.433\n",
      "Epoch 48, Train Accuracy: 0.41683333333333333\n",
      "Epoch 48, Dev Accuracy: 0.434\n",
      "Epoch 49, Train Accuracy: 0.4103333333333333\n",
      "Epoch 49, Dev Accuracy: 0.433\n",
      "Epoch 50, Train Accuracy: 0.42233333333333334\n",
      "Epoch 50, Dev Accuracy: 0.447\n",
      "Epoch 51, Train Accuracy: 0.441\n",
      "Epoch 51, Dev Accuracy: 0.4205\n",
      "Epoch 52, Train Accuracy: 0.44166666666666665\n",
      "Epoch 52, Dev Accuracy: 0.4415\n",
      "Epoch 53, Train Accuracy: 0.456\n",
      "Epoch 53, Dev Accuracy: 0.438\n",
      "Epoch 54, Train Accuracy: 0.4801666666666667\n",
      "Epoch 54, Dev Accuracy: 0.467\n",
      "Epoch 55, Train Accuracy: 0.49033333333333334\n",
      "Epoch 55, Dev Accuracy: 0.468\n",
      "Epoch 56, Train Accuracy: 0.49316666666666664\n",
      "Epoch 56, Dev Accuracy: 0.45\n",
      "Epoch 57, Train Accuracy: 0.5226666666666666\n",
      "Epoch 57, Dev Accuracy: 0.459\n",
      "Epoch 58, Train Accuracy: 0.5256666666666666\n",
      "Epoch 58, Dev Accuracy: 0.4585\n",
      "Epoch 59, Train Accuracy: 0.5376666666666666\n",
      "Epoch 59, Dev Accuracy: 0.453\n",
      "Epoch 60, Train Accuracy: 0.538\n",
      "Epoch 60, Dev Accuracy: 0.457\n",
      "Epoch 61, Train Accuracy: 0.5583333333333333\n",
      "Epoch 61, Dev Accuracy: 0.4565\n",
      "Epoch 62, Train Accuracy: 0.5606666666666666\n",
      "Epoch 62, Dev Accuracy: 0.4635\n",
      "Epoch 63, Train Accuracy: 0.5616666666666666\n",
      "Epoch 63, Dev Accuracy: 0.462\n",
      "Epoch 64, Train Accuracy: 0.5665\n",
      "Epoch 64, Dev Accuracy: 0.4625\n",
      "Epoch 65, Train Accuracy: 0.5686666666666667\n",
      "Epoch 65, Dev Accuracy: 0.4695\n",
      "Epoch 66, Train Accuracy: 0.5735\n",
      "Epoch 66, Dev Accuracy: 0.4635\n",
      "Epoch 67, Train Accuracy: 0.5731666666666667\n",
      "Epoch 67, Dev Accuracy: 0.4675\n",
      "Epoch 68, Train Accuracy: 0.5835\n",
      "Epoch 68, Dev Accuracy: 0.4695\n",
      "Epoch 69, Train Accuracy: 0.5828333333333333\n",
      "Epoch 69, Dev Accuracy: 0.481\n",
      "Epoch 70, Train Accuracy: 0.5893333333333334\n",
      "Epoch 70, Dev Accuracy: 0.4775\n",
      "Epoch 71, Train Accuracy: 0.5918333333333333\n",
      "Epoch 71, Dev Accuracy: 0.479\n",
      "Epoch 72, Train Accuracy: 0.5946666666666667\n",
      "Epoch 72, Dev Accuracy: 0.4795\n",
      "Epoch 73, Train Accuracy: 0.5961666666666666\n",
      "Epoch 73, Dev Accuracy: 0.487\n",
      "Epoch 74, Train Accuracy: 0.6045\n",
      "Epoch 74, Dev Accuracy: 0.492\n",
      "Epoch 75, Train Accuracy: 0.605\n",
      "Epoch 75, Dev Accuracy: 0.4935\n",
      "Epoch 76, Train Accuracy: 0.6088333333333333\n",
      "Epoch 76, Dev Accuracy: 0.4945\n",
      "Epoch 77, Train Accuracy: 0.6143333333333333\n",
      "Epoch 77, Dev Accuracy: 0.5005\n",
      "Epoch 78, Train Accuracy: 0.6235\n",
      "Epoch 78, Dev Accuracy: 0.5005\n",
      "Epoch 79, Train Accuracy: 0.6213333333333333\n",
      "Epoch 79, Dev Accuracy: 0.5065\n",
      "Epoch 80, Train Accuracy: 0.6253333333333333\n",
      "Epoch 80, Dev Accuracy: 0.502\n",
      "Epoch 81, Train Accuracy: 0.6385\n",
      "Epoch 81, Dev Accuracy: 0.4985\n",
      "Epoch 82, Train Accuracy: 0.6418333333333334\n",
      "Epoch 82, Dev Accuracy: 0.5065\n",
      "Epoch 83, Train Accuracy: 0.6358333333333334\n",
      "Epoch 83, Dev Accuracy: 0.511\n",
      "Epoch 84, Train Accuracy: 0.647\n",
      "Epoch 84, Dev Accuracy: 0.5105\n",
      "Epoch 85, Train Accuracy: 0.646\n",
      "Epoch 85, Dev Accuracy: 0.521\n",
      "Epoch 86, Train Accuracy: 0.6538333333333334\n",
      "Epoch 86, Dev Accuracy: 0.5155\n",
      "Epoch 87, Train Accuracy: 0.6548333333333334\n",
      "Epoch 87, Dev Accuracy: 0.5175\n",
      "Epoch 88, Train Accuracy: 0.6601666666666667\n",
      "Epoch 88, Dev Accuracy: 0.5175\n",
      "Epoch 89, Train Accuracy: 0.6623333333333333\n",
      "Epoch 89, Dev Accuracy: 0.5205\n",
      "Epoch 90, Train Accuracy: 0.6678333333333333\n",
      "Epoch 90, Dev Accuracy: 0.5165\n",
      "Epoch 91, Train Accuracy: 0.6675\n",
      "Epoch 91, Dev Accuracy: 0.526\n",
      "Epoch 92, Train Accuracy: 0.6765\n",
      "Epoch 92, Dev Accuracy: 0.524\n",
      "Epoch 93, Train Accuracy: 0.6728333333333333\n",
      "Epoch 93, Dev Accuracy: 0.524\n",
      "Epoch 94, Train Accuracy: 0.6795\n",
      "Epoch 94, Dev Accuracy: 0.524\n",
      "Epoch 95, Train Accuracy: 0.681\n",
      "Epoch 95, Dev Accuracy: 0.52\n",
      "Epoch 96, Train Accuracy: 0.6803333333333333\n",
      "Epoch 96, Dev Accuracy: 0.5255\n",
      "Epoch 97, Train Accuracy: 0.6828333333333333\n",
      "Epoch 97, Dev Accuracy: 0.527\n",
      "Epoch 98, Train Accuracy: 0.6851666666666667\n",
      "Epoch 98, Dev Accuracy: 0.5215\n",
      "Epoch 99, Train Accuracy: 0.6818333333333333\n",
      "Epoch 99, Dev Accuracy: 0.5265\n",
      "Epoch 100, Train Accuracy: 0.6921666666666667\n",
      "Epoch 100, Dev Accuracy: 0.527\n",
      "Epoch 101, Train Accuracy: 0.691\n",
      "Epoch 101, Dev Accuracy: 0.528\n",
      "Epoch 102, Train Accuracy: 0.6941666666666667\n",
      "Epoch 102, Dev Accuracy: 0.5365\n",
      "Epoch 103, Train Accuracy: 0.7021666666666667\n",
      "Epoch 103, Dev Accuracy: 0.535\n",
      "Epoch 104, Train Accuracy: 0.6965\n",
      "Epoch 104, Dev Accuracy: 0.5355\n",
      "Epoch 105, Train Accuracy: 0.7061666666666667\n",
      "Epoch 105, Dev Accuracy: 0.5375\n",
      "Epoch 106, Train Accuracy: 0.708\n",
      "Epoch 106, Dev Accuracy: 0.5395\n",
      "Epoch 107, Train Accuracy: 0.7105\n",
      "Epoch 107, Dev Accuracy: 0.5345\n",
      "Epoch 108, Train Accuracy: 0.7098333333333333\n",
      "Epoch 108, Dev Accuracy: 0.538\n",
      "Epoch 109, Train Accuracy: 0.7113333333333334\n",
      "Epoch 109, Dev Accuracy: 0.5385\n",
      "Epoch 110, Train Accuracy: 0.72\n",
      "Epoch 110, Dev Accuracy: 0.537\n",
      "Epoch 111, Train Accuracy: 0.7175\n",
      "Epoch 111, Dev Accuracy: 0.539\n",
      "Epoch 112, Train Accuracy: 0.7206666666666667\n",
      "Epoch 112, Dev Accuracy: 0.538\n",
      "Epoch 113, Train Accuracy: 0.7243333333333334\n",
      "Epoch 113, Dev Accuracy: 0.5415\n",
      "Epoch 114, Train Accuracy: 0.7283333333333334\n",
      "Epoch 114, Dev Accuracy: 0.539\n",
      "Epoch 115, Train Accuracy: 0.7206666666666667\n",
      "Epoch 115, Dev Accuracy: 0.539\n",
      "Epoch 116, Train Accuracy: 0.7263333333333334\n",
      "Epoch 116, Dev Accuracy: 0.5425\n",
      "Epoch 117, Train Accuracy: 0.7295\n",
      "Epoch 117, Dev Accuracy: 0.546\n",
      "Epoch 118, Train Accuracy: 0.7293333333333333\n",
      "Epoch 118, Dev Accuracy: 0.5495\n",
      "Epoch 119, Train Accuracy: 0.7335\n",
      "Epoch 119, Dev Accuracy: 0.544\n",
      "Epoch 120, Train Accuracy: 0.7323333333333333\n",
      "Epoch 120, Dev Accuracy: 0.543\n",
      "Epoch 121, Train Accuracy: 0.7321666666666666\n",
      "Epoch 121, Dev Accuracy: 0.5505\n",
      "Epoch 122, Train Accuracy: 0.7381666666666666\n",
      "Epoch 122, Dev Accuracy: 0.547\n",
      "Epoch 123, Train Accuracy: 0.7351666666666666\n",
      "Epoch 123, Dev Accuracy: 0.55\n",
      "Epoch 124, Train Accuracy: 0.7435\n",
      "Epoch 124, Dev Accuracy: 0.5505\n",
      "Epoch 125, Train Accuracy: 0.7415\n",
      "Epoch 125, Dev Accuracy: 0.5525\n",
      "Epoch 126, Train Accuracy: 0.7423333333333333\n",
      "Epoch 126, Dev Accuracy: 0.5505\n",
      "Epoch 127, Train Accuracy: 0.7498333333333334\n",
      "Epoch 127, Dev Accuracy: 0.561\n",
      "Epoch 128, Train Accuracy: 0.7443333333333333\n",
      "Epoch 128, Dev Accuracy: 0.5545\n",
      "Epoch 129, Train Accuracy: 0.7411666666666666\n",
      "Epoch 129, Dev Accuracy: 0.554\n",
      "Epoch 130, Train Accuracy: 0.7453333333333333\n",
      "Epoch 130, Dev Accuracy: 0.559\n",
      "Epoch 131, Train Accuracy: 0.7426666666666667\n",
      "Epoch 131, Dev Accuracy: 0.5495\n",
      "Epoch 132, Train Accuracy: 0.7383333333333333\n",
      "Epoch 132, Dev Accuracy: 0.552\n",
      "Epoch 133, Train Accuracy: 0.7411666666666666\n",
      "Epoch 133, Dev Accuracy: 0.5525\n",
      "Epoch 134, Train Accuracy: 0.7378333333333333\n",
      "Epoch 134, Dev Accuracy: 0.5515\n",
      "Epoch 135, Train Accuracy: 0.7331666666666666\n",
      "Epoch 135, Dev Accuracy: 0.5505\n",
      "Epoch 136, Train Accuracy: 0.7291666666666666\n",
      "Epoch 136, Dev Accuracy: 0.5475\n",
      "Epoch 137, Train Accuracy: 0.7313333333333333\n",
      "Epoch 137, Dev Accuracy: 0.546\n",
      "Epoch 138, Train Accuracy: 0.7363333333333333\n",
      "Epoch 138, Dev Accuracy: 0.553\n",
      "Epoch 139, Train Accuracy: 0.7461666666666666\n",
      "Epoch 139, Dev Accuracy: 0.5475\n",
      "Epoch 140, Train Accuracy: 0.7475\n",
      "Epoch 140, Dev Accuracy: 0.555\n",
      "Epoch 141, Train Accuracy: 0.7461666666666666\n",
      "Epoch 141, Dev Accuracy: 0.5535\n",
      "Epoch 142, Train Accuracy: 0.7531666666666667\n",
      "Epoch 142, Dev Accuracy: 0.5565\n",
      "Epoch 143, Train Accuracy: 0.7528333333333334\n",
      "Epoch 143, Dev Accuracy: 0.557\n",
      "Epoch 144, Train Accuracy: 0.7578333333333334\n",
      "Epoch 144, Dev Accuracy: 0.5555\n",
      "Epoch 145, Train Accuracy: 0.7561666666666667\n",
      "Epoch 145, Dev Accuracy: 0.549\n",
      "Epoch 146, Train Accuracy: 0.7568333333333334\n",
      "Epoch 146, Dev Accuracy: 0.5525\n",
      "Epoch 147, Train Accuracy: 0.7481666666666666\n",
      "Epoch 147, Dev Accuracy: 0.5525\n",
      "Epoch 148, Train Accuracy: 0.7503333333333333\n",
      "Epoch 148, Dev Accuracy: 0.5495\n",
      "Epoch 149, Train Accuracy: 0.7411666666666666\n",
      "Epoch 149, Dev Accuracy: 0.5505\n",
      "Epoch 150, Train Accuracy: 0.736\n",
      "Epoch 150, Dev Accuracy: 0.5385\n",
      "Epoch 151, Train Accuracy: 0.7366666666666667\n",
      "Epoch 151, Dev Accuracy: 0.5305\n",
      "Epoch 152, Train Accuracy: 0.7451666666666666\n",
      "Epoch 152, Dev Accuracy: 0.5345\n",
      "Epoch 153, Train Accuracy: 0.7553333333333333\n",
      "Epoch 153, Dev Accuracy: 0.544\n",
      "Epoch 154, Train Accuracy: 0.755\n",
      "Epoch 154, Dev Accuracy: 0.5415\n",
      "Epoch 155, Train Accuracy: 0.7631666666666667\n",
      "Epoch 155, Dev Accuracy: 0.548\n",
      "Epoch 156, Train Accuracy: 0.77\n",
      "Epoch 156, Dev Accuracy: 0.553\n",
      "Epoch 157, Train Accuracy: 0.768\n",
      "Epoch 157, Dev Accuracy: 0.544\n",
      "Epoch 158, Train Accuracy: 0.7718333333333334\n",
      "Epoch 158, Dev Accuracy: 0.553\n",
      "Epoch 159, Train Accuracy: 0.778\n",
      "Epoch 159, Dev Accuracy: 0.5455\n",
      "Epoch 160, Train Accuracy: 0.7785\n",
      "Epoch 160, Dev Accuracy: 0.5525\n",
      "Epoch 161, Train Accuracy: 0.7805\n",
      "Epoch 161, Dev Accuracy: 0.554\n",
      "Epoch 162, Train Accuracy: 0.779\n",
      "Epoch 162, Dev Accuracy: 0.551\n",
      "Epoch 163, Train Accuracy: 0.7785\n",
      "Epoch 163, Dev Accuracy: 0.5545\n",
      "Epoch 164, Train Accuracy: 0.7828333333333334\n",
      "Epoch 164, Dev Accuracy: 0.554\n",
      "Epoch 165, Train Accuracy: 0.7816666666666666\n",
      "Epoch 165, Dev Accuracy: 0.5495\n",
      "Epoch 166, Train Accuracy: 0.7835\n",
      "Epoch 166, Dev Accuracy: 0.549\n",
      "Epoch 167, Train Accuracy: 0.782\n",
      "Epoch 167, Dev Accuracy: 0.553\n",
      "Epoch 168, Train Accuracy: 0.7861666666666667\n",
      "Epoch 168, Dev Accuracy: 0.5515\n",
      "Epoch 169, Train Accuracy: 0.7846666666666666\n",
      "Epoch 169, Dev Accuracy: 0.5535\n",
      "Epoch 170, Train Accuracy: 0.7858333333333334\n",
      "Epoch 170, Dev Accuracy: 0.5515\n",
      "Epoch 171, Train Accuracy: 0.7861666666666667\n",
      "Epoch 171, Dev Accuracy: 0.546\n",
      "Epoch 172, Train Accuracy: 0.7823333333333333\n",
      "Epoch 172, Dev Accuracy: 0.544\n",
      "Epoch 173, Train Accuracy: 0.7835\n",
      "Epoch 173, Dev Accuracy: 0.542\n",
      "Epoch 174, Train Accuracy: 0.7855\n",
      "Epoch 174, Dev Accuracy: 0.5375\n",
      "Epoch 175, Train Accuracy: 0.7806666666666666\n",
      "Epoch 175, Dev Accuracy: 0.543\n",
      "Epoch 176, Train Accuracy: 0.7836666666666666\n",
      "Epoch 176, Dev Accuracy: 0.5425\n",
      "Epoch 177, Train Accuracy: 0.7765\n",
      "Epoch 177, Dev Accuracy: 0.5345\n",
      "Epoch 178, Train Accuracy: 0.7775\n",
      "Epoch 178, Dev Accuracy: 0.5325\n",
      "Epoch 179, Train Accuracy: 0.768\n",
      "Epoch 179, Dev Accuracy: 0.5335\n",
      "Epoch 180, Train Accuracy: 0.7553333333333333\n",
      "Epoch 180, Dev Accuracy: 0.5495\n",
      "Epoch 181, Train Accuracy: 0.7541666666666667\n",
      "Epoch 181, Dev Accuracy: 0.562\n",
      "Epoch 182, Train Accuracy: 0.7588333333333334\n",
      "Epoch 182, Dev Accuracy: 0.556\n",
      "Epoch 183, Train Accuracy: 0.761\n",
      "Epoch 183, Dev Accuracy: 0.556\n",
      "Epoch 184, Train Accuracy: 0.7745\n",
      "Epoch 184, Dev Accuracy: 0.5625\n",
      "Epoch 185, Train Accuracy: 0.783\n",
      "Epoch 185, Dev Accuracy: 0.563\n",
      "Epoch 186, Train Accuracy: 0.7765\n",
      "Epoch 186, Dev Accuracy: 0.5635\n",
      "Epoch 187, Train Accuracy: 0.7815\n",
      "Epoch 187, Dev Accuracy: 0.5645\n",
      "Epoch 188, Train Accuracy: 0.785\n",
      "Epoch 188, Dev Accuracy: 0.573\n",
      "Epoch 189, Train Accuracy: 0.79\n",
      "Epoch 189, Dev Accuracy: 0.5695\n",
      "Epoch 190, Train Accuracy: 0.7965\n",
      "Epoch 190, Dev Accuracy: 0.567\n",
      "Epoch 191, Train Accuracy: 0.8046666666666666\n",
      "Epoch 191, Dev Accuracy: 0.571\n",
      "Epoch 192, Train Accuracy: 0.8023333333333333\n",
      "Epoch 192, Dev Accuracy: 0.571\n",
      "Epoch 193, Train Accuracy: 0.8048333333333333\n",
      "Epoch 193, Dev Accuracy: 0.5675\n",
      "Epoch 194, Train Accuracy: 0.8066666666666666\n",
      "Epoch 194, Dev Accuracy: 0.567\n",
      "Epoch 195, Train Accuracy: 0.8128333333333333\n",
      "Epoch 195, Dev Accuracy: 0.5715\n",
      "Epoch 196, Train Accuracy: 0.8116666666666666\n",
      "Epoch 196, Dev Accuracy: 0.569\n",
      "Epoch 197, Train Accuracy: 0.8071666666666667\n",
      "Epoch 197, Dev Accuracy: 0.568\n",
      "Epoch 198, Train Accuracy: 0.8083333333333333\n",
      "Epoch 198, Dev Accuracy: 0.5725\n",
      "Epoch 199, Train Accuracy: 0.8088333333333333\n",
      "Epoch 199, Dev Accuracy: 0.574\n",
      "Epoch 200, Train Accuracy: 0.8116666666666666\n",
      "Epoch 200, Dev Accuracy: 0.568\n"
     ]
    }
   ],
   "source": [
    "# trains the model on the training set and saves the model weights after each epoch in the modelWeights folder\n",
    "train(songs_train, labels_train, songs_dev, labels_dev)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
